{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==2.11.0\n!pip install emoji","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport re\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\n# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import accuracy_score,matthews_corrcoef\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm, trange,tnrange,tqdm_notebook\nimport random\nimport os\nimport io\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport itertools\nimport emoji\nimport unicodedata\nimport matplotlib.pyplot as plt\nimport transformers\nprint(transformers.__version__)\nprint(torch.__version__)\n#% matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# identify and specify the GPU as the device, later in training loop we will load data into devic'''e\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\nSEED = 19\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == torch.device(\"cuda\"):\n    torch.cuda.manual_seed_all(SEED)\n    \nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BertTokenizer to run end-to-end tokenization: punctuation splitting + word piece. \nBertForSequenceClassification is the Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output). \nBertConfig is the configuration class to store model configurations. \nAdamW implements Adam learning rate optimization algorithm, it is a type of Stochastic Gradient Descent with momentum. Here momentum is described as the moving average of the gradient instead of gradient itself.\nget_linear_schedule_with_warmup creates a schedule with a learning rate that decreases linearly after linearly increasing during a warm-up period.","metadata":{}},{"cell_type":"code","source":"def load_dict_contractions():\n    return {\n        \"u\":\"you\",\n        \"doin\":\"doing\",\n        \"jus\":\"just\",\n        \"jst\":\"just\",\n        \"waitin\":\"waiting\",\n        \"whatchu\":\"what are you\",\n        \"wtf\":\"what the fuck\",\n        \"ugh\": \"disappointment\",\n        \"omg\": \"oh my god\", \n        \"lol\":\"laugh out loud\",\n        \"iv\":\"i have\",\n        \"let's\":\"let us\",\n        \"dont\":\"do not\",\n        \"2\":\"to\",\n        \"im\": \"i am\",\n        \"ain't\":\"is not\",\n        \"amn't\":\"am not\",\n        \"aren't\":\"are not\",\n        \"can't\":\"can not\",\n        \"cant\":\"can not\",\n        \"'cause\":\"because\",\n        \"couldn't\":\"could not\",\n        \"couldn't've\":\"could not have\",\n        \"could've\":\"could have\",\n        \"daren't\":\"dare not\",\n        \"daresn't\":\"dare not\",\n        \"dasn't\":\"dare not\",\n        \"didn't\":\"did not\",\n        \"doesn't\":\"does not\",\n        \"don't\":\"do not\",\n        \"e'er\":\"ever\",\n        \"em\":\"them\",\n        \"everyone's\":\"everyone is\",\n        \"finna\":\"fixing to\",\n        \"gimme\":\"give me\",\n        \"gonna\":\"going to\",\n        \"gon't\":\"go not\",\n        \"gotta\":\"got to\",\n        \"hadn't\":\"had not\",\n        \"hasn't\":\"has not\",\n        \"haven't\":\"have not\",\n        \"he'd\":\"he would\",\n        \"he'll\":\"he will\",\n        \"he's\":\"he is\",\n        \"he've\":\"he have\",\n        \"how'd\":\"how would\",\n        \"how'll\":\"how will\",\n        \"how're\":\"how are\",\n        \"how's\":\"how is\",\n        \"I'd\":\"I would\",\n        \"I'll\":\"I will\",\n        \"I'm\":\"I am\",\n        \"I'm'a\":\"I am about to\",\n        \"I'm'o\":\"I am going to\",\n        \"isn't\":\"is not\",\n        \"it'd\":\"it would\",\n        \"it'll\":\"it will\",\n        \"it's\":\"it is\",\n        \"I've\":\"I have\",\n        \"kinda\":\"kind of\",\n        \"let's\":\"let us\",\n        \"mayn't\":\"may not\",\n        \"may've\":\"may have\",\n        \"mightn't\":\"might not\",\n        \"might've\":\"might have\",\n        \"mustn't\":\"must not\",\n        \"mustn't've\":\"must not have\",\n        \"must've\":\"must have\",\n        \"needn't\":\"need not\",\n        \"ne'er\":\"never\",\n        \"o'\":\"of\",\n        \"o'er\":\"over\",\n        \"ol'\":\"old\",\n        \"oughtn't\":\"ought not\",\n        \"shalln't\":\"shall not\",\n        \"shan't\":\"shall not\",\n        \"she'd\":\"she would\",\n        \"she'll\":\"she will\",\n        \"she's\":\"she is\",\n        \"shouldn't\":\"should not\",\n        \"shouldn't've\":\"should not have\",\n        \"should've\":\"should have\",\n        \"somebody's\":\"somebody is\",\n        \"someone's\":\"someone is\",\n        \"something's\":\"something is\",\n        \"that'd\":\"that would\",\n        \"that'll\":\"that will\",\n        \"that're\":\"that are\",\n        \"that's\":\"that is\",\n        \"there'd\":\"there would\",\n        \"there'll\":\"there will\",\n        \"there're\":\"there are\",\n        \"there's\":\"there is\",\n        \"these're\":\"these are\",\n        \"they'd\":\"they would\",\n        \"they'll\":\"they will\",\n        \"they're\":\"they are\",\n        \"they've\":\"they have\",\n        \"this's\":\"this is\",\n        \"those're\":\"those are\",\n        \"'tis\":\"it is\",\n        \"'twas\":\"it was\",\n        \"wanna\":\"want to\",\n        \"wasn't\":\"was not\",\n        \"we'd\":\"we would\",\n        \"we'd've\":\"we would have\",\n        \"we'll\":\"we will\",\n        \"we're\":\"we are\",\n        \"weren't\":\"were not\",\n        \"we've\":\"we have\",\n        \"what'd\":\"what did\",\n        \"what'll\":\"what will\",\n        \"what're\":\"what are\",\n        \"what's\":\"what is\",\n        \"what've\":\"what have\",\n        \"when's\":\"when is\",\n        \"where'd\":\"where did\",\n        \"where're\":\"where are\",\n        \"where's\":\"where is\",\n        \"where've\":\"where have\",\n        \"which's\":\"which is\",\n        \"who'd\":\"who would\",\n        \"who'd've\":\"who would have\",\n        \"who'll\":\"who will\",\n        \"who're\":\"who are\",\n        \"who's\":\"who is\",\n        \"who've\":\"who have\",\n        \"why'd\":\"why did\",\n        \"why're\":\"why are\",\n        \"why's\":\"why is\",\n        \"won't\":\"will not\",\n        \"wouldn't\":\"would not\",\n        \"would've\":\"would have\",\n        \"y'all\":\"you all\",\n        \"you'd\":\"you would\",\n        \"you'll\":\"you will\",\n        \"you're\":\"you are\",\n        \"you've\":\"you have\",\n        \"Whatcha\":\"What are you\",\n        \"luv\":\"love\",\n        \"sux\":\"sucks\"\n        }\nglobal CONTRACTIONS\nCONTRACTIONS = load_dict_contractions()\nCONTRACTIONS = {k.lower():v.lower() for k, v in CONTRACTIONS.items()}\nCONTRACTIONS1 = {}\nfor k, v in CONTRACTIONS.items():\n  if \"'\" in k:\n    CONTRACTIONS1[k.replace(\"'\", '')] = v\n  else:\n    pass\nCONTRACTIONS.update(CONTRACTIONS1)\n\n\ndef tweet_cleaning_for_sentiment_analysis(tweet):    \n    tweet = tweet.replace(\"â€™\",\"'\")\n    tweet = re.sub(r\"http[^\\s]+\", \" \", tweet)\n    tweet = re.sub(r\"https[^\\s]+\", \" \", tweet)\n    tweet = re.sub(r\"RT @\\S+\", \" \", tweet)\n    tweet = re.sub(r\"rt @\\S+\", \" \", tweet)\n    tweet = re.sub(r\"rt\", \" \", tweet)\n    tweet = re.sub(r\"Rt\", \" \", tweet)\n    tweet = re.sub(r\"#\\w+\", \" \", tweet)\n    tweet = tweet.lower()\n    words = tweet.split()\n    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n    tweet = \" \".join(reformed)\n    \n    #Deal with emojis\n    tweet = emoji.demojize(tweet)\n    #tweet = re.sub(r\"http+\", \"\", tweet)\n\n    #tweet = tweet.replace(\":\",\" \")\n    #Removal of Punctuation\n    #tweet = re.sub(r'[^a-zA-z\\s\\t]', '', tweet)    \n\n\n    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9']+\", ' ', str(tweet).lower()).strip()\n    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n    text = ' '.join(text.split())\n    return text\n\n\ndef preprocess(text, stem=False):\n    # Remove link,user and special characters\n    #text = re.sub(\"@\\S+|https?:\\S+|http?:\\S\", ' ', str(text).lower()).strip()\n    #text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n    text = ' '.join(text.split())\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tweet_cleaning_for_sentiment_analysis(\"this is good ðŸ˜€ the\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/emotion-dataset-1/Semeval_2007.csv', encoding =\"UTF-8\")\ndf = df[['text', 'sentiments']]\nprint(df.shape)\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndf = df.dropna()\na = [True if i>2 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf1 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=5)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/emotion_dataset1_1.csv', encoding =\"UTF-8\")\ndf = df[['text', 'sentiments']]\nprint(df.shape)\ndf = df.dropna()\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\na = [True if i>2 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf2 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=5)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/tweet_clean.csv', encoding =\"UTF-8\")\ndf = df[['text', 'sentiments']]\nprint(df.columns)\ndf = df.dropna()\ndf = df[df['sentiments'] != 'disgust']\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndf = df.dropna()\na = [True if i>2 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf3 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=5)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/text_emotion.csv', encoding =\"UTF-8\", names = ['sentiments', '1', 'text'])[1:]\ndf = df[['text', 'sentiments']]\ndf = df.dropna()\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndelete = [\"empty\", \"enthusiasm\", \"boredom\", \"anger\", \"neutral\"]\na = [False if i in delete else True for i in df['sentiments'].values]\ndf = df[a]\ndf = df.dropna()\na = [True if i>2 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf4 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=9)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/Semeval_2007.csv', encoding =\"UTF-8\")\ndf = df[['text', 'sentiments']]\nprint(df.shape)\ndf = df.dropna()\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndf = df.dropna()\na = [True if i>2 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf5 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=5)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/Emotion Phrases.csv', encoding =\"UTF-8\", names=['sentiments', 'text'])\ndf = df[['text', 'sentiments']]\nprint(df.head(5))\ndf = df.dropna()\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndf = df.dropna()\na = [True if i>2 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf6 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=7)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/text-classification.csv', encoding =\"UTF-8\")\ndf = df[['text', 'sentiments']]\nprint(df.head(5))\ndf = df.dropna()\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndf = df.dropna()\na = [True if i>2 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf7 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=5)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/DATA.csv', encoding='iso-8859-1')\ndf = df[['text', 'sentiments']]\nprint(df.head(5))\ndf = df.dropna()\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndf = df.dropna()\na = [True if i>2 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf8 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=7)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/sadness_happiness_love_surprise.csv', encoding='iso-8859-1')\ndf = df[['text', 'sentiments']]\nprint(df.head(5))\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\nprint(df.head(5))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndf = df.dropna()\na = [True if i>2 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf9 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=4)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/neutral.csv', encoding='iso-8859-1')\ndf = df[['text', 'sentiments']]\nprint(df.head(5))\ndf = df.dropna()\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\nprint(df.head(5))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndf = df.dropna()\na = [True if i>2 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf10 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=4)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/Hashtag_data.csv', encoding='iso-8859-1')\ndf = df[['text', 'sentiments']]\nprint(df.head(5))\ndf = df.dropna()\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\nprint(df.head(5))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndf = df.dropna()\na = [True if i>3 else False for i in df['len'].values]\ndf = df[a]\na = [True if i<50 else False for i in df['len'].values]\ndf = df[a]\ndf11 = df\nprint(df.sentiments.value_counts())\nfig, axes = plt.subplots(nrows=1, ncols=6)\nfor index, senti in enumerate(set(df.sentiments.values)):\n    df[df['sentiments']==senti].groupby(['len']).size().plot(ax=axes[index], title=senti) \nplt.show()\n\n#df_h1 = df[df['sentiments'] != 'happy'][:5000]\n#df_h2 = df[df['sentiments'] == 'sadness'][:5000]\ndf_h3 = df[df['sentiments'] == 'anger'][:5000]\ndf_h4 = df[df['sentiments'] == 'fear'][:5000]\ndf_h5 = df[df['sentiments'] == 'love'][:5000]\n#df_h6 = df[df['sentiments'] == 'surprise'][:5000]\ndf_h = pd.concat([df_h3, df_h4, df_h5])\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset-1/tested_correct.csv', encoding='iso-8859-1')\ndf = df[['text', 'sentiments']]\ndf = df.dropna()\ndf['text'] = df.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf.text = df.text.apply(lambda x: preprocess(x))\nprint(df.head(5))\ndf['len'] = df.text.apply(lambda x: len(x.split()))\ndf = df.dropna()\ndf7_ = df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([ df2, df3, df4, df5, df6, df7, df8, df9, df10, df_h])\nprint(df.sentiments.value_counts())\ndf = df.drop_duplicates(subset=[\"text\"])\ndf.loc[df.sentiments == \"sad\", \"sentiments\"] = \"sadness\"\ndf.loc[(df.sentiments == \"joy\")|(df.sentiments == \"Happy\"), \"sentiments\"] = \"happy\"\ndf = df[df['sentiments'] != 'worry']\ndf.loc[df.sentiments == \"happiness\", \"sentiments\"] = \"happy\"\ndf = df[(df['sentiments'] != 'disgust')&(df['sentiments'] != 'shame')&(df['sentiments'] != 'guilt')]\ndf.loc[(df.sentiments == \"fun\")|(df.sentiments == \"relief\"), \"sentiments\"] = \"happy\"\ndf.loc[df.sentiments == \"hate\", \"sentiments\"] = \"anger\"\n#df.loc[df.sentiments == \"surprise\", \"sentiments\"] = \"love\"\n#df.loc[df.sentiments == \"love\", \"sentiments\"] = \"excited\"\nprint(df.head(5))\nprint(df.sentiments.value_counts())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''number_category = 7000\ndf1_ = df[df['sentiments']=='happy']\ndf2_ = df[df['sentiments']=='sadness']\ndf3_ = df[df['sentiments']=='excited']\ndf4_ = df[df['sentiments']=='anger']\ndf5_ = df[df['sentiments']=='fear']\ndf6_ = df[df['sentiments']=='neutral']\ndf7_ = df[df['sentiments']=='love']\ndf_ = pd.concat([df1_, df2_, df3_, df4_, df5_, df6_, df7_])\ndf_.to_csv('a.csv')'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in set(df.sentiments.values):\n    print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.sentiments.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnumber_category = 7000\ndf1_ = df[df['sentiments']=='happy'][:]\ndf2_ = df[df['sentiments']=='sadness'][:10000]\ndf3_ = df[df['sentiments']=='love']\ndf4_ = df[df['sentiments']=='anger'][:10000]\ndf5_ = df[df['sentiments']=='fear'][:10000]\ndf6_ = df[df['sentiments']=='neutral']\ndf_ = pd.concat([df1_, df2_, df3_, df4_, df5_, df6_, df7_])\ndf_.loc[(df_.sentiments == \"happy\")|(df_.sentiments == \"love\"), \"sentiments\"] = \"positive\"\ndf_.loc[(df_.sentiments == \"sadness\")|(df_.sentiments == \"anger\")|(df_.sentiments == \"fear\"), \"sentiments\"] = \"negative\"\ndf_ = df_[df_['sentiments']!='excited']\ndf_ = df_[df_['sentiments']!='positive']\ndf_ = df_[df_['sentiments']!='negative']\n\ndf_neg_pos = pd.read_csv('/kaggle/input/emotion-dataset-1/Sentiment_50000.csv')\ndf_neg_pos = df_neg_pos[['sentiments', 'text']]\ndf_neg_pos.loc[(df_neg_pos.sentiments == 0), \"sentiments\"] = \"negative\"\ndf_neg_pos.loc[(df_neg_pos.sentiments == 1), \"sentiments\"] = \"positive\"\n\n\ndf_BTC = pd.read_csv('/kaggle/input/emotion-dataset-1/BTC_tweets.csv')\ndf_BTC = df_BTC[['sentiments', 'text']]\n\ndf_1 = pd.concat([df_, df_neg_pos, df_BTC])\ndf_1['text'] = df_1.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf_1.text = df_1.text.apply(lambda x: preprocess(x))\ndf_1 = df_1[['sentiments', 'text']]\ndf_1 = df_1.dropna()\ndf_1 = df_1.drop_duplicates(subset=[\"text\"])\nprint(df_1.sentiments.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_1 = pd.concat([df_, df_neg_pos, df_BTC])\ndf_1['text'] = df_1.text.apply(tweet_cleaning_for_sentiment_analysis)\ndf_1.text = df_1.text.apply(lambda x: preprocess(x))\nprint(df_1.sentiments.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_1 = df_1[['sentiments', 'text']]\ndf_1 = df_1.dropna()\n#df_1 = df_1.drop_duplicates(subset=[\"text\"])\nprint(df_1.sentiments.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_1\nprint(df_1.sentiments.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns={'sentiments':'label'},inplace=True)\ndf.rename(columns={'text':'sentence'},inplace=True)\ndf.to_csv('a-22-11-20.csv')\ndf['label'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf['label_enc'] = labelencoder.fit_transform(df['label'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['label','label_enc']].drop_duplicates(keep='first')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.label.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns={'label':'label_desc'},inplace=True)\ndf.rename(columns={'label_enc':'label'},inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## create label and sentence list\nsentences = df.sentence.values\n\n#check distribution of data based on labels\nprint(\"Distribution of data based on labels: \",df.label.value_counts())\n\n# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n# In the original paper, the authors used a length of 512.\nMAX_LEN = 256\n\n## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\ninput_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\nlabels = df.label.values\n\nprint(\"Actual sentence before tokenization: \",sentences[2])\nprint(\"Encoded Input from dataset: \",input_ids[2])\n\n## Create attention mask\nattention_masks = []\n## Create a mask of 1 for all input tokens and 0 for all padding tokens\nattention_masks = [[float(i>0) for i in seq] for seq in input_ids]\nprint(attention_masks[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Prep for training","metadata":{}},{"cell_type":"markdown","source":"#### Split into a training set and a test set using a stratified k fold","metadata":{}},{"cell_type":"code","source":"train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\ntrain_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert all our data into torch tensors, required data type for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\ntrain_data = TensorDataset(train_inputs,train_masks,train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\nvalidation_sampler = RandomSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3).to(device)\n\n# Parameters:\nlr = 2e-5\nadam_epsilon = 1e-8\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs =3\n\nnum_warmup_steps = 0\nnum_training_steps = len(train_dataloader)*epochs\n\n### In Transformers, optimizer and schedules are splitted and instantiated like this:\noptimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Store our loss and accuracy for plotting\ntrain_loss_set = []\nlearning_rate = []\ny_text_pred = []\ny_text_orig = []\n\n# Gradients gets accumulated by default\nmodel.zero_grad()\n\n# tnrange is a tqdm wrapper around the normal python range\nfor _ in tnrange(1,epochs+1,desc='Epoch'):\n  print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n  # Calculate total loss for this epoch\n  batch_loss = 0\n\n  for step, batch in enumerate(train_dataloader):\n    # Set our model to training mode (as opposed to evaluation mode)\n    model.train()\n    \n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n\n    # Forward pass\n    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n    loss = outputs[0]\n    \n    # Backward pass\n    loss.backward()\n    \n    # Clip the norm of the gradients to 1.0\n    # Gradient clipping is not in AdamW anymore\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    \n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n    \n    # Update learning rate schedule\n    scheduler.step()\n\n    # Clear the previous accumulated gradients\n    optimizer.zero_grad()\n    \n    # Update tracking variables\n    batch_loss += loss.item()\n\n  # Calculate the average loss over the training data.\n  avg_train_loss = batch_loss / len(train_dataloader)\n\n  #store the current learning rate\n  for param_group in optimizer.param_groups:\n    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n    learning_rate.append(param_group['lr'])\n    \n  train_loss_set.append(avg_train_loss)\n  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n    \n  # Validation\n\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n\n  # Tracking variables \n  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n\n  # Evaluate data for one epoch\n  for batch in validation_dataloader:\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    \n    # Move logits and labels to CPU\n    logits = logits[0].to('cpu').numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    pred_flat = np.argmax(logits, axis=1).flatten()\n    labels_flat = label_ids.flatten()\n    y_text_orig = y_text_orig + list(labels_flat)\n    y_text_pred = y_text_pred + list(pred_flat)\n    \n    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n    \n    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n    \n    eval_accuracy += tmp_eval_accuracy\n    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n    nb_eval_steps += 1\n\n  print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n  print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['label','label_desc']].drop_duplicates(keep='first')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## emotion labels\n\nlabel2int = { \"negative\": 0, \"neutral\": 1, \"positive\": 2}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" df_metrics['Predicted_class'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values, target_names=label2int.keys(), digits=len(label2int)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n## emotion labels\n#{ \"anger\": 0, \"fear\": 1, \"happy\": 2, \"love\":3, \"neutral\": 4, \"sadness\": 5}\nlabel2int = { \"negative\": 0, \"neutral\": 1, \"positive\": 2}\nint2label = { 0: \"negative\", 1:\"neutral\", 2:\"positive\"}\n#y_text_pred = df_metrics['Predicted_class'].values\n#y_text_orig = df_metrics['Actual_class'].values\n\n#y_text_pred = [int2label[i] for i in y_text_pred]\n#y_text_orig = [int2label[i] for i in y_text_orig]\n#print((y_text_orig))\n#print((y_text_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=30)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n    plt.yticks(tick_marks, classes, fontsize=22)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=25)\n    plt.xlabel('Predicted label', fontsize=25)\n\ncnf_matrix = confusion_matrix(y_text_orig, y_text_pred)\nplt.figure(figsize=(12,12))\n#{ \"anger\": 0, \"fear\": 1, \"happy\": 2, \"love\":3, \"neutral\": 4, \"sadness\": 5}\nplot_confusion_matrix(cnf_matrix, classes=[\"negative\", \"neutral\", \"positive\"], title=\"Confusion matrix\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## emotion labels\nlabe = {\n  \"negative\": 0,\n  \"neutral\": 1,\n  \"positive\": 2\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load('/kaggle/input/sentiment-model-bin/Sentiment_model-21-1-16.bin', map_location=device).to(device)\ntokenizer = torch.load('/kaggle/input/sentiment-model-bin/tokenizer-21-1-16.bin', map_location=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Emotion_model(sentences, model, tokenizer):\n    label = [\"negative\", \"neutral\", \"positive\"]\n    model1 = model.eval()\n    input_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=256,pad_to_max_length=True) for sent in sentences]\n    input_masks = [[float(i>0) for i in seq] for seq in list(input_ids)]\n    #input_masks = input_masks.reshape([1,input_mask.shape])\n    b_input_ids = torch.tensor(input_ids).to(device)\n    b_input_mask = torch.tensor(input_masks).to(device)\n    with torch.no_grad():\n        outputs = model1(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    #print(F.softmax(outputs[0]).to('cpu').numpy()[0])\n    print(['{:f}'.format(item) for item in F.softmax(outputs[0], dim=1).to('cpu').numpy()[0]*100])\n    logits = outputs[0].to('cpu').numpy()\n    pred_flat = np.argmax(logits, axis=1).flatten()\n    return label[pred_flat[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tweet_cleaning_for_sentiment_analysis1(tweet):    \n    tweet = tweet.replace(\"â€™\",\"'\")\n    tweet = re.sub(r\"http[^\\s]+\", \" \", tweet)\n    tweet = re.sub(r\"https[^\\s]+\", \" \", tweet)\n    #tweet = re.sub(r\"[.,!]\", \" \", tweet)\n    tweet = tweet.lower()\n    words = tweet.split()\n    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n    tweet = \" \".join(reformed)\n    \n    #Deal with emojis\n    tweet = emoji.demojize(tweet)  \n\n    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    text = re.sub(\"https?:\\S+|http?:\\S|[^A-Za-z0-9@#']+\", ' ', str(tweet).lower()).strip()\n    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n    text = ' '.join(text.split())\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = [\"\"\"i am thrilled by you. \"\"\"]\nsentences = [tweet_cleaning_for_sentiment_analysis1(sentences[0])]\nprint([\"  Negative \", \"  Neutral \", \"   Positive  \"])\nprint(Emotion_model(sentences, model, tokenizer))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model,'Sentiment_model50000-22-12-09.bin')\ntorch.save(tokenizer,'tokenizer50000-22-12-09.bin')\ndf.to_csv('a50000-22-12-09.csv')\n#df.to_csv('a-21-1-16.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('a-22-10-23.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nlabel = [\"anger\", \"fear\", \"happy\" , \"love\", \"neutral\", \"sadness\"]\ndf = pd.read_csv('/kaggle/input/emotion-analysis/Emotion  Sentiment analysis - Emotions.csv', encoding='iso-8859-1')\npredict_neu = []\nfor sentences in df.Statements.values:\n    sentences = [tweet_cleaning_for_sentiment_analysis1(sentences)]\n    model = model.eval()\n    predict_neu.append(Emotion_model(sentences, model, tokenizer))\ndf['predict'] = predict_neu\ndf.to_csv('predict_neu_lov_20_10.csv')\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}